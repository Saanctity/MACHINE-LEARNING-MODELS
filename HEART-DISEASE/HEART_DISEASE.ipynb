{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjw3SENjXo5C"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Heart Disease Detection System\n",
        "================================\n",
        "Predicts if a patient has heart disease using patient vitals and medical data.\n",
        "Optimized for Google Colab with <5 minute runtime.\n",
        "\"\"\"\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"HEART DISEASE DETECTION SYSTEM\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 1: LOAD DATASET\n",
        "# ============================================================================\n",
        "print(\"\\n[1/7] Loading Dataset...\")\n",
        "\n",
        "# Upload dataset from local system\n",
        "from google.colab import files\n",
        "print(\"Please upload your heart disease dataset (CSV file):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the filename\n",
        "filename = list(uploaded.keys())[0]\n",
        "df = pd.read_csv(filename)\n",
        "\n",
        "print(f\"‚úì Dataset loaded successfully: {filename}\")\n",
        "print(f\"  Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n"
      ],
      "metadata": {
        "id": "NvaSfCM5fSL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 2: EXPLORATORY DATA ANALYSIS (EDA)\n",
        "# ============================================================================\n",
        "print(\"\\n[2/7] Exploratory Data Analysis...\")\n",
        "\n",
        "# Display basic information\n",
        "print(\"\\n--- Dataset Info ---\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n--- First 5 Rows ---\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n--- Statistical Summary ---\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\n--- Missing Values ---\")\n",
        "missing = df.isnull().sum()\n",
        "if missing.sum() == 0:\n",
        "    print(\"‚úì No missing values found\")\n",
        "else:\n",
        "    print(missing[missing > 0])\n",
        "\n",
        "print(\"\\n--- Target Distribution ---\")\n",
        "print(df['target'].value_counts())\n",
        "print(f\"Class Balance: {df['target'].value_counts(normalize=True).to_dict()}\")\n",
        "\n",
        "# Visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Target distribution\n",
        "df['target'].value_counts().plot(kind='bar', ax=axes[0, 0], color=['#2ecc71', '#e74c3c'])\n",
        "axes[0, 0].set_title('Target Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Class (0=Normal, 1=Heart Disease)')\n",
        "axes[0, 0].set_ylabel('Count')\n",
        "axes[0, 0].set_xticklabels(['Normal', 'Heart Disease'], rotation=0)\n",
        "\n",
        "# Age distribution by target\n",
        "df.boxplot(column='age', by='target', ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Age Distribution by Target', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Target')\n",
        "axes[0, 1].set_ylabel('Age (years)')\n",
        "\n",
        "# Correlation heatmap\n",
        "corr_matrix = df.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
        "            ax=axes[1, 0], cbar_kws={'shrink': 0.8})\n",
        "axes[1, 0].set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Feature importance preview (using correlation with target)\n",
        "target_corr = corr_matrix['target'].abs().sort_values(ascending=False)[1:]\n",
        "target_corr.plot(kind='barh', ax=axes[1, 1], color='skyblue')\n",
        "axes[1, 1].set_title('Feature Correlation with Target', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Absolute Correlation')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úì EDA completed\")\n"
      ],
      "metadata": {
        "id": "pGdRHqeyfWr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 3: DATA PREPROCESSING\n",
        "# ============================================================================\n",
        "print(\"\\n[3/7] Data Preprocessing...\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split dataset (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"  Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "# Feature Scaling (important for SVM and Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"‚úì Data preprocessing completed\")\n"
      ],
      "metadata": {
        "id": "cVjqwQyYfcEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 4: MODEL TRAINING\n",
        "# ============================================================================\n",
        "print(\"\\n[4/7] Training Multiple Models...\")\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    'Support Vector Machine': SVC(probability=True, random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n  Training {name}...\")\n",
        "\n",
        "    # Use scaled data for LR and SVM, original for tree-based\n",
        "    if name in ['Logistic Regression', 'Support Vector Machine']:\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    results[name] = {\n",
        "        'model': model,\n",
        "        'accuracy': accuracy,\n",
        "        'roc_auc': roc_auc,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }\n",
        "\n",
        "    print(f\"    Accuracy: {accuracy:.4f} | ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "print(\"\\n‚úì Model training completed\")\n"
      ],
      "metadata": {
        "id": "K4TULWXAffvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 5: MODEL EVALUATION\n",
        "# ============================================================================\n",
        "print(\"\\n[5/7] Evaluating Models...\")\n",
        "\n",
        "# Find best model\n",
        "best_model_name = max(results, key=lambda x: results[x]['accuracy'])\n",
        "best_model = results[best_model_name]['model']\n",
        "best_accuracy = results[best_model_name]['accuracy']\n",
        "\n",
        "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
        "print(f\"   Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"   ROC-AUC: {results[best_model_name]['roc_auc']:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(f\"\\n--- Classification Report ({best_model_name}) ---\")\n",
        "print(classification_report(y_test, results[best_model_name]['y_pred'],\n",
        "                          target_names=['Normal', 'Heart Disease']))\n",
        "\n",
        "# Visualizations\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# 1. Model Comparison\n",
        "model_names = list(results.keys())\n",
        "accuracies = [results[m]['accuracy'] for m in model_names]\n",
        "roc_aucs = [results[m]['roc_auc'] for m in model_names]\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.35\n",
        "\n",
        "axes[0].bar(x - width/2, accuracies, width, label='Accuracy', color='#3498db')\n",
        "axes[0].bar(x + width/2, roc_aucs, width, label='ROC-AUC', color='#e74c3c')\n",
        "axes[0].set_xlabel('Models', fontweight='bold')\n",
        "axes[0].set_ylabel('Score', fontweight='bold')\n",
        "axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels(model_names, rotation=45, ha='right')\n",
        "axes[0].legend()\n",
        "axes[0].set_ylim([0.5, 1.0])\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 2. Confusion Matrix\n",
        "cm = confusion_matrix(y_test, results[best_model_name]['y_pred'])\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
        "            xticklabels=['Normal', 'Heart Disease'],\n",
        "            yticklabels=['Normal', 'Heart Disease'])\n",
        "axes[1].set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('True Label', fontweight='bold')\n",
        "axes[1].set_xlabel('Predicted Label', fontweight='bold')\n",
        "\n",
        "# 3. ROC Curve\n",
        "for name in results:\n",
        "    fpr, tpr, _ = roc_curve(y_test, results[name]['y_pred_proba'])\n",
        "    axes[2].plot(fpr, tpr, label=f\"{name} (AUC={results[name]['roc_auc']:.3f})\", linewidth=2)\n",
        "\n",
        "axes[2].plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
        "axes[2].set_xlabel('False Positive Rate', fontweight='bold')\n",
        "axes[2].set_ylabel('True Positive Rate', fontweight='bold')\n",
        "axes[2].set_title('ROC Curves', fontsize=14, fontweight='bold')\n",
        "axes[2].legend(loc='lower right')\n",
        "axes[2].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WVrmNYBzfkKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 6: FEATURE IMPORTANCE (for best model if tree-based)\n",
        "# ============================================================================\n",
        "print(\"\\n[6/7] Analyzing Feature Importance...\")\n",
        "\n",
        "if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'importance': best_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(\"\\n--- Top 10 Important Features ---\")\n",
        "    print(feature_importance.head(10))\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(data=feature_importance.head(10), x='importance', y='feature', palette='viridis')\n",
        "    plt.title(f'Top 10 Feature Importance - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Importance Score', fontweight='bold')\n",
        "    plt.ylabel('Features', fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úì Feature importance analysis completed\")\n"
      ],
      "metadata": {
        "id": "fdMqDW1-fpQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 7: PREDICTION FUNCTION\n",
        "# ============================================================================\n",
        "print(\"\\n[7/7] Setting Up Prediction System...\")\n",
        "\n",
        "def predict_heart_disease(patient_data):\n",
        "    \"\"\"\n",
        "    Predict heart disease for a new patient\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    patient_data : dict or pd.DataFrame\n",
        "        Patient features (same as training data)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    prediction : int (0 or 1)\n",
        "    probability : float (0 to 1)\n",
        "    \"\"\"\n",
        "    if isinstance(patient_data, dict):\n",
        "        patient_data = pd.DataFrame([patient_data])\n",
        "\n",
        "    if best_model_name in ['Logistic Regression', 'Support Vector Machine']:\n",
        "        patient_scaled = scaler.transform(patient_data)\n",
        "        prediction = best_model.predict(patient_scaled)[0]\n",
        "        probability = best_model.predict_proba(patient_scaled)[0][1]\n",
        "    else:\n",
        "        prediction = best_model.predict(patient_data)[0]\n",
        "        probability = best_model.predict_proba(patient_data)[0][1]\n",
        "\n",
        "    return prediction, probability\n",
        "\n",
        "# Example prediction\n",
        "print(\"\\n--- Example Prediction ---\")\n",
        "sample_patient = X_test.iloc[0].to_dict()\n",
        "pred, prob = predict_heart_disease(sample_patient)\n",
        "\n",
        "print(f\"Patient Data: {sample_patient}\")\n",
        "print(f\"Prediction: {'Heart Disease' if pred == 1 else 'Normal'}\")\n",
        "print(f\"Probability: {prob:.2%}\")\n",
        "print(f\"Actual: {'Heart Disease' if y_test.iloc[0] == 1 else 'Normal'}\")\n",
        "\n",
        "print(\"\\n‚úì Prediction system ready\")\n"
      ],
      "metadata": {
        "id": "L0P1-jxJfuTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"‚úì Dataset: {df.shape[0]} patients, {df.shape[1]} features\")\n",
        "print(f\"‚úì Best Model: {best_model_name}\")\n",
        "print(f\"‚úì Test Accuracy: {best_accuracy:.2%}\")\n",
        "print(f\"‚úì ROC-AUC Score: {results[best_model_name]['roc_auc']:.4f}\")\n",
        "print(f\"‚úì Model ready for predictions!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Save the best model (optional)\n",
        "print(\"\\nüí° Tip: You can use 'predict_heart_disease()' function for new predictions\")\n",
        "print(\"üí° All models and results are stored in the 'results' dictionary\")"
      ],
      "metadata": {
        "id": "CKnGsOTGfyqW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}